{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\Pose\"\n",
    "\n",
    "target = ['front', 'back', 'left', 'right']\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "for l in target:\n",
    "    for img in os.listdir(os.path.join(dir, l)):\n",
    "        imgs += [os.path.join(dir, l, img)]\n",
    "        labels += [l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'front': 35, 'back': 34, 'left': 35, 'right': 37})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0027_c4_f0023987.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0030_c4_f0026288.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0033_c2_f0061704.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0042_c2_f0063664.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0051_c4_f0030830.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0078_c4_f0034045.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0079_c2_f0066874.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0114_c6_f0061083.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0127_c2_f0075875.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0135_c4_f0046746.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0143_c4_f0049743.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0147_c4_f0049659.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0151_c4_f0051330.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0162_c4_f0053991.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\0167_c2_f0084455.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4405_c6_f0090766.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4494_c6_f0107868.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4503_c6_f0109212.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4511_c6_f0111629.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4521_c6_f0116273.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4540_c6_f0120241.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4550_c6_f0123830.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4568_c6_f0131328.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4580_c6_f0134270.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4582_c6_f0135822.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4594_c6_f0139332.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4596_c6_f0139920.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4613_c6_f0142855.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4634_c6_f0146277.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4646_c6_f0148110.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4647_c6_f0148026.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4739_c6_f0173250.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4743_c6_f0176222.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\4782_c6_f0188800.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\front\\\\7139_c1_f0156370.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0023_c4_f0031504.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0025_c1_f0053650.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0046_c4_f0039327.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0047_c4_f0039318.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0050_c4_f0039649.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0066_c6_f0041602.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0068_c4_f0042531.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0075_c4_f0043898.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0078_c1_f0065952.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0089_c8_f0025415.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0090_c2_f0073203.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0091_c2_f0073387.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0103_c2_f0070498.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0103_c5_f0073085.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0103_c6_f0049155.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0112_c1_f0073159.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0112_c2_f0072749.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0112_c5_f0074748.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\0112_c6_f0050859.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\2772_c5_f0222630.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\3358_c7_f0069570.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\3515_c6_f0086755.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\3561_c6_f0100189.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\3609_c6_f0119994.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\3731_c5_f0190369.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\3731_c6_f0164095.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\3763_c5_f0208701.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\4060_c6_f0035171.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\4071_c8_f0010885.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\4079_c6_f0040114.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\4082_c6_f0040940.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\4100_c6_f0044284.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\4134_c6_f0049764.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\back\\\\4141_c8_f0022224.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0042_c5_f0068874.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0043_c6_f0036866.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0043_c7_f0041987.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0046_c2_f0064337.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0050_c2_f0064798.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0050_c3_f0041757.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0053_c1_f0060659.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0053_c2_f0063910.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0068_c1_f0063910.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0068_c2_f0068064.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0068_c3_f0044983.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0075_c3_f0046520.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0083_c1_f0067852.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0088_c1_f0068997.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0088_c2_f0072748.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0088_c3_f0050799.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0090_c1_f0069115.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0095_c1_f0069660.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0095_c2_f0073463.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0095_c5_f0078456.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0098_c5_f0078829.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0111_c1_f0071894.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\0111_c6_f0049513.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\2001_c2_f0205882.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\2407_c3_f0031171.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\2736_c6_f0159549.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\3201_c5_f0179605.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\3410_c5_f0084555.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\3446_c5_f0095876.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\3446_c8_f0046676.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\3495_c5_f0107184.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\3618_c5_f0148084.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\3649_c7_f0167398.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\3731_c7_f0169587.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\left\\\\3761_c6_f0169189.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0019_c6_f0040430.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0027_c1_f0054309.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0027_c2_f0054638.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0027_c3_f0030042.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0030_c2_f0056822.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0030_c3_f0031952.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0034_c1_f0057636.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0035_c1_f0057672.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0051_c3_f0035958.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0066_c1_f0063339.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0072_c2_f0064014.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0086_c2_f0067260.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0086_c3_f0042905.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0097_c1_f0069940.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0099_c1_f0070437.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0099_c2_f0069951.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0099_c3_f0045400.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0106_c2_f0071469.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0106_c3_f0046940.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0119_c2_f0073254.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0123_c2_f0075327.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\0123_c3_f0050437.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\1811_c3_f0135556.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\1834_c3_f0137933.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\1834_c5_f0165887.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\2023_c2_f0210412.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\2023_c3_f0185641.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\2053_c2_f0222845.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\2053_c3_f0197416.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\2454_c6_f0060381.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\4070_c8_f0016049.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\4116_c8_f0024630.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\4143_c7_f0053383.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\4144_c8_f0029854.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\4190_c8_f0038517.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\4204_c7_f0064275.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\Desktop\\\\Pose\\\\right\\\\4257_c7_f0072650.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19.792934    14.7630825    0.7199818 ]\n",
      " [ 21.776253    12.397593     0.67566746]\n",
      " [ 17.446198    12.588003     0.6967247 ]\n",
      " [ 25.646606    12.870308     0.78534216]\n",
      " [ 14.204609    13.26079      0.74512255]\n",
      " [ 32.168194    24.775505     0.97032887]\n",
      " [  9.924631    25.385803     0.9769603 ]\n",
      " [ 38.786545    38.302593     0.8991085 ]\n",
      " [  2.4704173   38.093758     0.76294357]\n",
      " [ 32.113293    47.578293     0.77951366]\n",
      " [ 11.11957     45.89802      0.62511295]\n",
      " [ 29.258831    55.198666     0.9779419 ]\n",
      " [ 15.127205    55.6004       0.99817747]\n",
      " [ 27.748482    78.34921      0.9242825 ]\n",
      " [ 17.233473    78.37921      0.94345474]\n",
      " [ 27.00557    102.67316      0.88032126]\n",
      " [ 17.763308   101.88202      0.905094  ]\n",
      " [ 21.046413    25.080654     0.9736446 ]]\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import openpifpaf\n",
    "import numpy as np\n",
    "predictor = openpifpaf.Predictor(checkpoint='shufflenetv2k16')\n",
    "pil_im = PIL.Image.open(imgs[0]).convert('RGB')\n",
    "predictions, _, _ = predictor.pil_image(pil_im)\n",
    "print(np.append(predictions[0].data, (predictions[0].data[5] + predictions[0].data[6])/2).reshape(-1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A =\\\n",
    "[\n",
    "    [1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], #11\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1], #12\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0], #13\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0], #14\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], #15\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], #16\n",
    "    [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]  #17\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): GCN_layer(\n",
      "      (fc): Linear(in_features=3, out_features=4, bias=True)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): GCN_layer(\n",
      "      (fc): Linear(in_features=4, out_features=8, bias=True)\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): GCN_layer(\n",
      "      (fc): Linear(in_features=8, out_features=16, bias=True)\n",
      "    )\n",
      "    (5): ReLU()\n",
      "    (6): GCN_layer(\n",
      "      (fc): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (7): ReLU()\n",
      "    (8): GCN_layer(\n",
      "      (fc): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (9): ReLU()\n",
      "    (10): Flatten(start_dim=1, end_dim=-1)\n",
      "    (11): Linear(in_features=288, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "learning_rate : 0.001\n",
      "batch_size : 30\n",
      "epochs : 50\n"
     ]
    }
   ],
   "source": [
    "from datasets import TrainSkeletonData\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import GCN\n",
    "import torch.nn as  nn\n",
    "from torchvision import transforms\n",
    "\n",
    "dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\pose-angle\"\n",
    "\n",
    "\n",
    "\n",
    "A =\\\n",
    "[\n",
    "    [1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], #11\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1], #12\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0], #13\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0], #14\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], #15\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], #16\n",
    "    [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]  #17\n",
    "]\n",
    "\n",
    "A = torch.tensor(A).float()\n",
    "in_feature = 3\n",
    "num_class = 4\n",
    "model = GCN(in_feature, num_class, A)\n",
    "print(model)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size= 30\n",
    "epochs = 50\n",
    "print(f\"learning_rate : {learning_rate}\")\n",
    "print(f\"batch_size : {batch_size}\")\n",
    "print(f\"epochs : {epochs}\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss=  loss_fn(pred,y)\n",
    "\n",
    "        #Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 3 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_skeleton\n",
      "save file\n",
      "skeletons.npy done\n",
      "labels.npy done\n",
      "Epoch 1\n",
      "---------------------------------\n",
      "loss: 0.191717 [    0/ 1260]\n",
      "loss: 0.188456 [   90/ 1260]\n",
      "loss: 0.431835 [  180/ 1260]\n",
      "loss: 0.174800 [  270/ 1260]\n",
      "loss: 0.113048 [  360/ 1260]\n",
      "loss: 0.361208 [  450/ 1260]\n",
      "loss: 0.174585 [  540/ 1260]\n",
      "loss: 0.300218 [  630/ 1260]\n",
      "loss: 0.141597 [  720/ 1260]\n",
      "loss: 0.271741 [  810/ 1260]\n",
      "loss: 0.233096 [  900/ 1260]\n",
      "loss: 0.185821 [  990/ 1260]\n",
      "loss: 0.303585 [ 1080/ 1260]\n",
      "loss: 0.249685 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "---------------------------------\n",
      "loss: 0.094657 [    0/ 1260]\n",
      "loss: 0.476106 [   90/ 1260]\n",
      "loss: 0.382426 [  180/ 1260]\n",
      "loss: 0.282250 [  270/ 1260]\n",
      "loss: 0.156332 [  360/ 1260]\n",
      "loss: 0.217049 [  450/ 1260]\n",
      "loss: 0.274550 [  540/ 1260]\n",
      "loss: 0.113509 [  630/ 1260]\n",
      "loss: 0.120655 [  720/ 1260]\n",
      "loss: 0.288051 [  810/ 1260]\n",
      "loss: 0.140359 [  900/ 1260]\n",
      "loss: 0.183721 [  990/ 1260]\n",
      "loss: 0.252876 [ 1080/ 1260]\n",
      "loss: 0.223706 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "---------------------------------\n",
      "loss: 0.431257 [    0/ 1260]\n",
      "loss: 0.349912 [   90/ 1260]\n",
      "loss: 0.095406 [  180/ 1260]\n",
      "loss: 0.176289 [  270/ 1260]\n",
      "loss: 0.104106 [  360/ 1260]\n",
      "loss: 0.235931 [  450/ 1260]\n",
      "loss: 0.161146 [  540/ 1260]\n",
      "loss: 0.180042 [  630/ 1260]\n",
      "loss: 0.193507 [  720/ 1260]\n",
      "loss: 0.183996 [  810/ 1260]\n",
      "loss: 0.308673 [  900/ 1260]\n",
      "loss: 0.088921 [  990/ 1260]\n",
      "loss: 0.131281 [ 1080/ 1260]\n",
      "loss: 0.142846 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "---------------------------------\n",
      "loss: 0.130696 [    0/ 1260]\n",
      "loss: 0.132677 [   90/ 1260]\n",
      "loss: 0.250316 [  180/ 1260]\n",
      "loss: 0.157319 [  270/ 1260]\n",
      "loss: 0.130311 [  360/ 1260]\n",
      "loss: 0.212653 [  450/ 1260]\n",
      "loss: 0.354059 [  540/ 1260]\n",
      "loss: 0.279464 [  630/ 1260]\n",
      "loss: 0.313466 [  720/ 1260]\n",
      "loss: 0.290299 [  810/ 1260]\n",
      "loss: 0.123263 [  900/ 1260]\n",
      "loss: 0.068036 [  990/ 1260]\n",
      "loss: 0.053405 [ 1080/ 1260]\n",
      "loss: 0.138752 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "---------------------------------\n",
      "loss: 0.407647 [    0/ 1260]\n",
      "loss: 0.219814 [   90/ 1260]\n",
      "loss: 0.491373 [  180/ 1260]\n",
      "loss: 0.184767 [  270/ 1260]\n",
      "loss: 0.146116 [  360/ 1260]\n",
      "loss: 0.079993 [  450/ 1260]\n",
      "loss: 0.280883 [  540/ 1260]\n",
      "loss: 0.251655 [  630/ 1260]\n",
      "loss: 0.263051 [  720/ 1260]\n",
      "loss: 0.301482 [  810/ 1260]\n",
      "loss: 0.143130 [  900/ 1260]\n",
      "loss: 0.291556 [  990/ 1260]\n",
      "loss: 0.125000 [ 1080/ 1260]\n",
      "loss: 0.132484 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "---------------------------------\n",
      "loss: 0.145995 [    0/ 1260]\n",
      "loss: 0.272850 [   90/ 1260]\n",
      "loss: 0.126519 [  180/ 1260]\n",
      "loss: 0.136877 [  270/ 1260]\n",
      "loss: 0.270557 [  360/ 1260]\n",
      "loss: 0.330025 [  450/ 1260]\n",
      "loss: 0.073089 [  540/ 1260]\n",
      "loss: 0.118893 [  630/ 1260]\n",
      "loss: 0.263701 [  720/ 1260]\n",
      "loss: 0.211337 [  810/ 1260]\n",
      "loss: 0.274587 [  900/ 1260]\n",
      "loss: 0.112757 [  990/ 1260]\n",
      "loss: 0.217590 [ 1080/ 1260]\n",
      "loss: 0.325742 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "---------------------------------\n",
      "loss: 0.120915 [    0/ 1260]\n",
      "loss: 0.330289 [   90/ 1260]\n",
      "loss: 0.199024 [  180/ 1260]\n",
      "loss: 0.286341 [  270/ 1260]\n",
      "loss: 0.393327 [  360/ 1260]\n",
      "loss: 0.070495 [  450/ 1260]\n",
      "loss: 0.188894 [  540/ 1260]\n",
      "loss: 0.049864 [  630/ 1260]\n",
      "loss: 0.166035 [  720/ 1260]\n",
      "loss: 0.275340 [  810/ 1260]\n",
      "loss: 0.330837 [  900/ 1260]\n",
      "loss: 0.104341 [  990/ 1260]\n",
      "loss: 0.123458 [ 1080/ 1260]\n",
      "loss: 0.144522 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "---------------------------------\n",
      "loss: 0.243672 [    0/ 1260]\n",
      "loss: 0.094591 [   90/ 1260]\n",
      "loss: 0.115540 [  180/ 1260]\n",
      "loss: 0.169032 [  270/ 1260]\n",
      "loss: 0.091732 [  360/ 1260]\n",
      "loss: 0.192523 [  450/ 1260]\n",
      "loss: 0.306259 [  540/ 1260]\n",
      "loss: 0.117011 [  630/ 1260]\n",
      "loss: 0.194092 [  720/ 1260]\n",
      "loss: 0.087732 [  810/ 1260]\n",
      "loss: 0.101767 [  900/ 1260]\n",
      "loss: 0.130767 [  990/ 1260]\n",
      "loss: 0.295096 [ 1080/ 1260]\n",
      "loss: 0.306602 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "---------------------------------\n",
      "loss: 0.231938 [    0/ 1260]\n",
      "loss: 0.148233 [   90/ 1260]\n",
      "loss: 0.166444 [  180/ 1260]\n",
      "loss: 0.263181 [  270/ 1260]\n",
      "loss: 0.248113 [  360/ 1260]\n",
      "loss: 0.327964 [  450/ 1260]\n",
      "loss: 0.298936 [  540/ 1260]\n",
      "loss: 0.094761 [  630/ 1260]\n",
      "loss: 0.072064 [  720/ 1260]\n",
      "loss: 0.148504 [  810/ 1260]\n",
      "loss: 0.047528 [  900/ 1260]\n",
      "loss: 0.249598 [  990/ 1260]\n",
      "loss: 0.351325 [ 1080/ 1260]\n",
      "loss: 0.104511 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "---------------------------------\n",
      "loss: 0.228537 [    0/ 1260]\n",
      "loss: 0.056543 [   90/ 1260]\n",
      "loss: 0.197880 [  180/ 1260]\n",
      "loss: 0.136614 [  270/ 1260]\n",
      "loss: 0.209448 [  360/ 1260]\n",
      "loss: 0.133296 [  450/ 1260]\n",
      "loss: 0.146680 [  540/ 1260]\n",
      "loss: 0.307088 [  630/ 1260]\n",
      "loss: 0.190816 [  720/ 1260]\n",
      "loss: 0.086525 [  810/ 1260]\n",
      "loss: 0.145305 [  900/ 1260]\n",
      "loss: 0.159344 [  990/ 1260]\n",
      "loss: 0.303912 [ 1080/ 1260]\n",
      "loss: 0.253670 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 11\n",
      "---------------------------------\n",
      "loss: 0.221441 [    0/ 1260]\n",
      "loss: 0.061359 [   90/ 1260]\n",
      "loss: 0.188380 [  180/ 1260]\n",
      "loss: 0.361081 [  270/ 1260]\n",
      "loss: 0.185575 [  360/ 1260]\n",
      "loss: 0.330493 [  450/ 1260]\n",
      "loss: 0.489713 [  540/ 1260]\n",
      "loss: 0.249445 [  630/ 1260]\n",
      "loss: 0.262393 [  720/ 1260]\n",
      "loss: 0.132632 [  810/ 1260]\n",
      "loss: 0.111117 [  900/ 1260]\n",
      "loss: 0.302089 [  990/ 1260]\n",
      "loss: 0.101401 [ 1080/ 1260]\n",
      "loss: 0.482042 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 12\n",
      "---------------------------------\n",
      "loss: 0.414578 [    0/ 1260]\n",
      "loss: 0.207092 [   90/ 1260]\n",
      "loss: 0.218830 [  180/ 1260]\n",
      "loss: 0.184480 [  270/ 1260]\n",
      "loss: 0.328012 [  360/ 1260]\n",
      "loss: 0.216224 [  450/ 1260]\n",
      "loss: 0.094022 [  540/ 1260]\n",
      "loss: 0.222308 [  630/ 1260]\n",
      "loss: 0.355481 [  720/ 1260]\n",
      "loss: 0.198370 [  810/ 1260]\n",
      "loss: 0.208819 [  900/ 1260]\n",
      "loss: 0.187683 [  990/ 1260]\n",
      "loss: 0.230269 [ 1080/ 1260]\n",
      "loss: 0.114109 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 13\n",
      "---------------------------------\n",
      "loss: 0.309589 [    0/ 1260]\n",
      "loss: 0.090464 [   90/ 1260]\n",
      "loss: 0.201676 [  180/ 1260]\n",
      "loss: 0.125484 [  270/ 1260]\n",
      "loss: 0.180913 [  360/ 1260]\n",
      "loss: 0.214587 [  450/ 1260]\n",
      "loss: 0.155728 [  540/ 1260]\n",
      "loss: 0.089853 [  630/ 1260]\n",
      "loss: 0.268650 [  720/ 1260]\n",
      "loss: 0.156538 [  810/ 1260]\n",
      "loss: 0.121753 [  900/ 1260]\n",
      "loss: 0.126509 [  990/ 1260]\n",
      "loss: 0.191739 [ 1080/ 1260]\n",
      "loss: 0.111361 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 14\n",
      "---------------------------------\n",
      "loss: 0.140467 [    0/ 1260]\n",
      "loss: 0.084213 [   90/ 1260]\n",
      "loss: 0.219493 [  180/ 1260]\n",
      "loss: 0.144391 [  270/ 1260]\n",
      "loss: 0.234262 [  360/ 1260]\n",
      "loss: 0.059780 [  450/ 1260]\n",
      "loss: 0.153553 [  540/ 1260]\n",
      "loss: 0.299129 [  630/ 1260]\n",
      "loss: 0.136210 [  720/ 1260]\n",
      "loss: 0.088516 [  810/ 1260]\n",
      "loss: 0.227109 [  900/ 1260]\n",
      "loss: 0.229111 [  990/ 1260]\n",
      "loss: 0.391505 [ 1080/ 1260]\n",
      "loss: 0.269036 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 15\n",
      "---------------------------------\n",
      "loss: 0.422380 [    0/ 1260]\n",
      "loss: 0.168037 [   90/ 1260]\n",
      "loss: 0.169019 [  180/ 1260]\n",
      "loss: 0.194323 [  270/ 1260]\n",
      "loss: 0.254188 [  360/ 1260]\n",
      "loss: 0.177746 [  450/ 1260]\n",
      "loss: 0.127760 [  540/ 1260]\n",
      "loss: 0.225755 [  630/ 1260]\n",
      "loss: 0.077480 [  720/ 1260]\n",
      "loss: 0.131542 [  810/ 1260]\n",
      "loss: 0.100682 [  900/ 1260]\n",
      "loss: 0.111619 [  990/ 1260]\n",
      "loss: 0.091807 [ 1080/ 1260]\n",
      "loss: 0.316843 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 16\n",
      "---------------------------------\n",
      "loss: 0.090875 [    0/ 1260]\n",
      "loss: 0.094723 [   90/ 1260]\n",
      "loss: 0.099236 [  180/ 1260]\n",
      "loss: 0.235749 [  270/ 1260]\n",
      "loss: 0.111871 [  360/ 1260]\n",
      "loss: 0.190350 [  450/ 1260]\n",
      "loss: 0.108578 [  540/ 1260]\n",
      "loss: 0.128862 [  630/ 1260]\n",
      "loss: 0.247029 [  720/ 1260]\n",
      "loss: 0.140538 [  810/ 1260]\n",
      "loss: 0.254049 [  900/ 1260]\n",
      "loss: 0.224843 [  990/ 1260]\n",
      "loss: 0.095286 [ 1080/ 1260]\n",
      "loss: 0.103125 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 17\n",
      "---------------------------------\n",
      "loss: 0.079398 [    0/ 1260]\n",
      "loss: 0.279997 [   90/ 1260]\n",
      "loss: 0.150215 [  180/ 1260]\n",
      "loss: 0.114387 [  270/ 1260]\n",
      "loss: 0.271982 [  360/ 1260]\n",
      "loss: 0.113760 [  450/ 1260]\n",
      "loss: 0.164039 [  540/ 1260]\n",
      "loss: 0.139135 [  630/ 1260]\n",
      "loss: 0.017619 [  720/ 1260]\n",
      "loss: 0.242114 [  810/ 1260]\n",
      "loss: 0.133120 [  900/ 1260]\n",
      "loss: 0.150971 [  990/ 1260]\n",
      "loss: 0.348340 [ 1080/ 1260]\n",
      "loss: 0.293469 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 18\n",
      "---------------------------------\n",
      "loss: 0.313725 [    0/ 1260]\n",
      "loss: 0.244832 [   90/ 1260]\n",
      "loss: 0.246668 [  180/ 1260]\n",
      "loss: 0.175343 [  270/ 1260]\n",
      "loss: 0.148555 [  360/ 1260]\n",
      "loss: 0.427818 [  450/ 1260]\n",
      "loss: 0.202219 [  540/ 1260]\n",
      "loss: 0.236580 [  630/ 1260]\n",
      "loss: 0.202549 [  720/ 1260]\n",
      "loss: 0.147448 [  810/ 1260]\n",
      "loss: 0.267621 [  900/ 1260]\n",
      "loss: 0.192617 [  990/ 1260]\n",
      "loss: 0.181940 [ 1080/ 1260]\n",
      "loss: 0.299341 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 19\n",
      "---------------------------------\n",
      "loss: 0.161238 [    0/ 1260]\n",
      "loss: 0.184128 [   90/ 1260]\n",
      "loss: 0.213167 [  180/ 1260]\n",
      "loss: 0.154974 [  270/ 1260]\n",
      "loss: 0.255034 [  360/ 1260]\n",
      "loss: 0.152072 [  450/ 1260]\n",
      "loss: 0.041500 [  540/ 1260]\n",
      "loss: 0.204631 [  630/ 1260]\n",
      "loss: 0.201539 [  720/ 1260]\n",
      "loss: 0.247586 [  810/ 1260]\n",
      "loss: 0.135405 [  900/ 1260]\n",
      "loss: 0.130608 [  990/ 1260]\n",
      "loss: 0.082789 [ 1080/ 1260]\n",
      "loss: 0.228611 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 20\n",
      "---------------------------------\n",
      "loss: 0.066593 [    0/ 1260]\n",
      "loss: 0.257272 [   90/ 1260]\n",
      "loss: 0.247034 [  180/ 1260]\n",
      "loss: 0.224676 [  270/ 1260]\n",
      "loss: 0.166090 [  360/ 1260]\n",
      "loss: 0.330710 [  450/ 1260]\n",
      "loss: 0.316797 [  540/ 1260]\n",
      "loss: 0.232415 [  630/ 1260]\n",
      "loss: 0.043909 [  720/ 1260]\n",
      "loss: 0.198098 [  810/ 1260]\n",
      "loss: 0.168286 [  900/ 1260]\n",
      "loss: 0.130745 [  990/ 1260]\n",
      "loss: 0.128287 [ 1080/ 1260]\n",
      "loss: 0.164131 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 21\n",
      "---------------------------------\n",
      "loss: 0.297680 [    0/ 1260]\n",
      "loss: 0.151173 [   90/ 1260]\n",
      "loss: 0.282816 [  180/ 1260]\n",
      "loss: 0.161115 [  270/ 1260]\n",
      "loss: 0.231233 [  360/ 1260]\n",
      "loss: 0.187051 [  450/ 1260]\n",
      "loss: 0.312697 [  540/ 1260]\n",
      "loss: 0.203526 [  630/ 1260]\n",
      "loss: 0.063208 [  720/ 1260]\n",
      "loss: 0.116676 [  810/ 1260]\n",
      "loss: 0.254787 [  900/ 1260]\n",
      "loss: 0.077061 [  990/ 1260]\n",
      "loss: 0.117622 [ 1080/ 1260]\n",
      "loss: 0.239760 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 22\n",
      "---------------------------------\n",
      "loss: 0.210217 [    0/ 1260]\n",
      "loss: 0.056807 [   90/ 1260]\n",
      "loss: 0.095512 [  180/ 1260]\n",
      "loss: 0.221992 [  270/ 1260]\n",
      "loss: 0.205123 [  360/ 1260]\n",
      "loss: 0.286967 [  450/ 1260]\n",
      "loss: 0.187770 [  540/ 1260]\n",
      "loss: 0.195151 [  630/ 1260]\n",
      "loss: 0.140094 [  720/ 1260]\n",
      "loss: 0.118467 [  810/ 1260]\n",
      "loss: 0.303962 [  900/ 1260]\n",
      "loss: 0.125669 [  990/ 1260]\n",
      "loss: 0.346529 [ 1080/ 1260]\n",
      "loss: 0.243961 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 23\n",
      "---------------------------------\n",
      "loss: 0.072663 [    0/ 1260]\n",
      "loss: 0.361063 [   90/ 1260]\n",
      "loss: 0.179636 [  180/ 1260]\n",
      "loss: 0.078185 [  270/ 1260]\n",
      "loss: 0.150725 [  360/ 1260]\n",
      "loss: 0.066409 [  450/ 1260]\n",
      "loss: 0.253574 [  540/ 1260]\n",
      "loss: 0.257888 [  630/ 1260]\n",
      "loss: 0.162145 [  720/ 1260]\n",
      "loss: 0.101990 [  810/ 1260]\n",
      "loss: 0.117675 [  900/ 1260]\n",
      "loss: 0.087215 [  990/ 1260]\n",
      "loss: 0.348481 [ 1080/ 1260]\n",
      "loss: 0.156745 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 24\n",
      "---------------------------------\n",
      "loss: 0.209636 [    0/ 1260]\n",
      "loss: 0.151157 [   90/ 1260]\n",
      "loss: 0.123369 [  180/ 1260]\n",
      "loss: 0.464196 [  270/ 1260]\n",
      "loss: 0.295083 [  360/ 1260]\n",
      "loss: 0.173746 [  450/ 1260]\n",
      "loss: 0.167232 [  540/ 1260]\n",
      "loss: 0.175729 [  630/ 1260]\n",
      "loss: 0.032938 [  720/ 1260]\n",
      "loss: 0.316552 [  810/ 1260]\n",
      "loss: 0.251205 [  900/ 1260]\n",
      "loss: 0.164648 [  990/ 1260]\n",
      "loss: 0.251985 [ 1080/ 1260]\n",
      "loss: 0.136876 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 25\n",
      "---------------------------------\n",
      "loss: 0.153784 [    0/ 1260]\n",
      "loss: 0.143426 [   90/ 1260]\n",
      "loss: 0.260854 [  180/ 1260]\n",
      "loss: 0.111855 [  270/ 1260]\n",
      "loss: 0.172973 [  360/ 1260]\n",
      "loss: 0.245318 [  450/ 1260]\n",
      "loss: 0.098721 [  540/ 1260]\n",
      "loss: 0.234752 [  630/ 1260]\n",
      "loss: 0.155009 [  720/ 1260]\n",
      "loss: 0.211120 [  810/ 1260]\n",
      "loss: 0.326188 [  900/ 1260]\n",
      "loss: 0.126167 [  990/ 1260]\n",
      "loss: 0.249353 [ 1080/ 1260]\n",
      "loss: 0.304906 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 26\n",
      "---------------------------------\n",
      "loss: 0.088085 [    0/ 1260]\n",
      "loss: 0.151757 [   90/ 1260]\n",
      "loss: 0.170377 [  180/ 1260]\n",
      "loss: 0.089096 [  270/ 1260]\n",
      "loss: 0.076329 [  360/ 1260]\n",
      "loss: 0.095913 [  450/ 1260]\n",
      "loss: 0.113123 [  540/ 1260]\n",
      "loss: 0.165704 [  630/ 1260]\n",
      "loss: 0.172784 [  720/ 1260]\n",
      "loss: 0.200951 [  810/ 1260]\n",
      "loss: 0.333694 [  900/ 1260]\n",
      "loss: 0.350291 [  990/ 1260]\n",
      "loss: 0.151076 [ 1080/ 1260]\n",
      "loss: 0.145543 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 27\n",
      "---------------------------------\n",
      "loss: 0.411587 [    0/ 1260]\n",
      "loss: 0.242245 [   90/ 1260]\n",
      "loss: 0.086169 [  180/ 1260]\n",
      "loss: 0.124925 [  270/ 1260]\n",
      "loss: 0.146727 [  360/ 1260]\n",
      "loss: 0.220673 [  450/ 1260]\n",
      "loss: 0.079511 [  540/ 1260]\n",
      "loss: 0.432645 [  630/ 1260]\n",
      "loss: 0.092258 [  720/ 1260]\n",
      "loss: 0.043394 [  810/ 1260]\n",
      "loss: 0.251416 [  900/ 1260]\n",
      "loss: 0.147458 [  990/ 1260]\n",
      "loss: 0.121005 [ 1080/ 1260]\n",
      "loss: 0.116251 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 28\n",
      "---------------------------------\n",
      "loss: 0.159453 [    0/ 1260]\n",
      "loss: 0.082513 [   90/ 1260]\n",
      "loss: 0.150351 [  180/ 1260]\n",
      "loss: 0.256248 [  270/ 1260]\n",
      "loss: 0.168105 [  360/ 1260]\n",
      "loss: 0.080742 [  450/ 1260]\n",
      "loss: 0.065295 [  540/ 1260]\n",
      "loss: 0.275196 [  630/ 1260]\n",
      "loss: 0.095211 [  720/ 1260]\n",
      "loss: 0.189119 [  810/ 1260]\n",
      "loss: 0.306549 [  900/ 1260]\n",
      "loss: 0.161325 [  990/ 1260]\n",
      "loss: 0.247386 [ 1080/ 1260]\n",
      "loss: 0.303336 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 29\n",
      "---------------------------------\n",
      "loss: 0.095627 [    0/ 1260]\n",
      "loss: 0.088485 [   90/ 1260]\n",
      "loss: 0.178789 [  180/ 1260]\n",
      "loss: 0.019680 [  270/ 1260]\n",
      "loss: 0.148354 [  360/ 1260]\n",
      "loss: 0.105440 [  450/ 1260]\n",
      "loss: 0.100473 [  540/ 1260]\n",
      "loss: 0.315520 [  630/ 1260]\n",
      "loss: 0.235615 [  720/ 1260]\n",
      "loss: 0.078440 [  810/ 1260]\n",
      "loss: 0.149542 [  900/ 1260]\n",
      "loss: 0.207120 [  990/ 1260]\n",
      "loss: 0.330189 [ 1080/ 1260]\n",
      "loss: 0.094853 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 30\n",
      "---------------------------------\n",
      "loss: 0.087953 [    0/ 1260]\n",
      "loss: 0.270126 [   90/ 1260]\n",
      "loss: 0.216684 [  180/ 1260]\n",
      "loss: 0.291403 [  270/ 1260]\n",
      "loss: 0.271472 [  360/ 1260]\n",
      "loss: 0.142403 [  450/ 1260]\n",
      "loss: 0.088006 [  540/ 1260]\n",
      "loss: 0.043387 [  630/ 1260]\n",
      "loss: 0.205489 [  720/ 1260]\n",
      "loss: 0.123214 [  810/ 1260]\n",
      "loss: 0.199215 [  900/ 1260]\n",
      "loss: 0.153294 [  990/ 1260]\n",
      "loss: 0.135696 [ 1080/ 1260]\n",
      "loss: 0.119435 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 31\n",
      "---------------------------------\n",
      "loss: 0.143145 [    0/ 1260]\n",
      "loss: 0.095677 [   90/ 1260]\n",
      "loss: 0.108974 [  180/ 1260]\n",
      "loss: 0.026776 [  270/ 1260]\n",
      "loss: 0.090452 [  360/ 1260]\n",
      "loss: 0.393610 [  450/ 1260]\n",
      "loss: 0.095556 [  540/ 1260]\n",
      "loss: 0.264790 [  630/ 1260]\n",
      "loss: 0.094202 [  720/ 1260]\n",
      "loss: 0.119986 [  810/ 1260]\n",
      "loss: 0.124981 [  900/ 1260]\n",
      "loss: 0.101034 [  990/ 1260]\n",
      "loss: 0.090576 [ 1080/ 1260]\n",
      "loss: 0.202319 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 32\n",
      "---------------------------------\n",
      "loss: 0.083876 [    0/ 1260]\n",
      "loss: 0.287107 [   90/ 1260]\n",
      "loss: 0.123083 [  180/ 1260]\n",
      "loss: 0.235847 [  270/ 1260]\n",
      "loss: 0.025525 [  360/ 1260]\n",
      "loss: 0.170793 [  450/ 1260]\n",
      "loss: 0.131262 [  540/ 1260]\n",
      "loss: 0.151259 [  630/ 1260]\n",
      "loss: 0.150870 [  720/ 1260]\n",
      "loss: 0.110961 [  810/ 1260]\n",
      "loss: 0.155692 [  900/ 1260]\n",
      "loss: 0.262105 [  990/ 1260]\n",
      "loss: 0.088586 [ 1080/ 1260]\n",
      "loss: 0.142698 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 33\n",
      "---------------------------------\n",
      "loss: 0.167439 [    0/ 1260]\n",
      "loss: 0.195902 [   90/ 1260]\n",
      "loss: 0.059376 [  180/ 1260]\n",
      "loss: 0.344330 [  270/ 1260]\n",
      "loss: 0.099912 [  360/ 1260]\n",
      "loss: 0.411550 [  450/ 1260]\n",
      "loss: 0.059018 [  540/ 1260]\n",
      "loss: 0.100856 [  630/ 1260]\n",
      "loss: 0.332852 [  720/ 1260]\n",
      "loss: 0.123794 [  810/ 1260]\n",
      "loss: 0.074727 [  900/ 1260]\n",
      "loss: 0.087921 [  990/ 1260]\n",
      "loss: 0.096171 [ 1080/ 1260]\n",
      "loss: 0.180149 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 34\n",
      "---------------------------------\n",
      "loss: 0.164291 [    0/ 1260]\n",
      "loss: 0.139602 [   90/ 1260]\n",
      "loss: 0.244732 [  180/ 1260]\n",
      "loss: 0.076157 [  270/ 1260]\n",
      "loss: 0.169757 [  360/ 1260]\n",
      "loss: 0.227571 [  450/ 1260]\n",
      "loss: 0.265993 [  540/ 1260]\n",
      "loss: 0.211785 [  630/ 1260]\n",
      "loss: 0.229682 [  720/ 1260]\n",
      "loss: 0.151435 [  810/ 1260]\n",
      "loss: 0.148325 [  900/ 1260]\n",
      "loss: 0.199652 [  990/ 1260]\n",
      "loss: 0.109743 [ 1080/ 1260]\n",
      "loss: 0.249764 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 35\n",
      "---------------------------------\n",
      "loss: 0.076541 [    0/ 1260]\n",
      "loss: 0.072777 [   90/ 1260]\n",
      "loss: 0.222693 [  180/ 1260]\n",
      "loss: 0.074624 [  270/ 1260]\n",
      "loss: 0.263433 [  360/ 1260]\n",
      "loss: 0.391140 [  450/ 1260]\n",
      "loss: 0.167148 [  540/ 1260]\n",
      "loss: 0.062298 [  630/ 1260]\n",
      "loss: 0.105576 [  720/ 1260]\n",
      "loss: 0.181445 [  810/ 1260]\n",
      "loss: 0.131164 [  900/ 1260]\n",
      "loss: 0.213126 [  990/ 1260]\n",
      "loss: 0.197825 [ 1080/ 1260]\n",
      "loss: 0.146763 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 36\n",
      "---------------------------------\n",
      "loss: 0.250333 [    0/ 1260]\n",
      "loss: 0.269722 [   90/ 1260]\n",
      "loss: 0.258594 [  180/ 1260]\n",
      "loss: 0.253188 [  270/ 1260]\n",
      "loss: 0.083896 [  360/ 1260]\n",
      "loss: 0.364695 [  450/ 1260]\n",
      "loss: 0.222503 [  540/ 1260]\n",
      "loss: 0.043702 [  630/ 1260]\n",
      "loss: 0.137100 [  720/ 1260]\n",
      "loss: 0.125347 [  810/ 1260]\n",
      "loss: 0.212739 [  900/ 1260]\n",
      "loss: 0.131183 [  990/ 1260]\n",
      "loss: 0.112366 [ 1080/ 1260]\n",
      "loss: 0.148982 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 37\n",
      "---------------------------------\n",
      "loss: 0.075725 [    0/ 1260]\n",
      "loss: 0.071153 [   90/ 1260]\n",
      "loss: 0.204973 [  180/ 1260]\n",
      "loss: 0.068398 [  270/ 1260]\n",
      "loss: 0.126481 [  360/ 1260]\n",
      "loss: 0.224930 [  450/ 1260]\n",
      "loss: 0.292542 [  540/ 1260]\n",
      "loss: 0.141179 [  630/ 1260]\n",
      "loss: 0.191504 [  720/ 1260]\n",
      "loss: 0.251421 [  810/ 1260]\n",
      "loss: 0.105859 [  900/ 1260]\n",
      "loss: 0.155240 [  990/ 1260]\n",
      "loss: 0.085718 [ 1080/ 1260]\n",
      "loss: 0.133677 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 38\n",
      "---------------------------------\n",
      "loss: 0.116692 [    0/ 1260]\n",
      "loss: 0.208611 [   90/ 1260]\n",
      "loss: 0.100951 [  180/ 1260]\n",
      "loss: 0.223508 [  270/ 1260]\n",
      "loss: 0.091749 [  360/ 1260]\n",
      "loss: 0.266203 [  450/ 1260]\n",
      "loss: 0.162966 [  540/ 1260]\n",
      "loss: 0.265001 [  630/ 1260]\n",
      "loss: 0.067600 [  720/ 1260]\n",
      "loss: 0.199111 [  810/ 1260]\n",
      "loss: 0.268131 [  900/ 1260]\n",
      "loss: 0.125716 [  990/ 1260]\n",
      "loss: 0.337888 [ 1080/ 1260]\n",
      "loss: 0.183384 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 39\n",
      "---------------------------------\n",
      "loss: 0.226565 [    0/ 1260]\n",
      "loss: 0.149653 [   90/ 1260]\n",
      "loss: 0.158155 [  180/ 1260]\n",
      "loss: 0.212547 [  270/ 1260]\n",
      "loss: 0.154224 [  360/ 1260]\n",
      "loss: 0.057488 [  450/ 1260]\n",
      "loss: 0.127820 [  540/ 1260]\n",
      "loss: 0.283841 [  630/ 1260]\n",
      "loss: 0.177964 [  720/ 1260]\n",
      "loss: 0.196051 [  810/ 1260]\n",
      "loss: 0.141466 [  900/ 1260]\n",
      "loss: 0.254448 [  990/ 1260]\n",
      "loss: 0.139394 [ 1080/ 1260]\n",
      "loss: 0.418514 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 40\n",
      "---------------------------------\n",
      "loss: 0.076686 [    0/ 1260]\n",
      "loss: 0.023035 [   90/ 1260]\n",
      "loss: 0.281794 [  180/ 1260]\n",
      "loss: 0.063814 [  270/ 1260]\n",
      "loss: 0.310426 [  360/ 1260]\n",
      "loss: 0.251371 [  450/ 1260]\n",
      "loss: 0.169884 [  540/ 1260]\n",
      "loss: 0.029477 [  630/ 1260]\n",
      "loss: 0.275933 [  720/ 1260]\n",
      "loss: 0.033592 [  810/ 1260]\n",
      "loss: 0.060501 [  900/ 1260]\n",
      "loss: 0.262294 [  990/ 1260]\n",
      "loss: 0.094472 [ 1080/ 1260]\n",
      "loss: 0.106768 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 41\n",
      "---------------------------------\n",
      "loss: 0.287902 [    0/ 1260]\n",
      "loss: 0.040746 [   90/ 1260]\n",
      "loss: 0.119445 [  180/ 1260]\n",
      "loss: 0.080786 [  270/ 1260]\n",
      "loss: 0.157395 [  360/ 1260]\n",
      "loss: 0.211287 [  450/ 1260]\n",
      "loss: 0.167650 [  540/ 1260]\n",
      "loss: 0.078327 [  630/ 1260]\n",
      "loss: 0.106435 [  720/ 1260]\n",
      "loss: 0.081761 [  810/ 1260]\n",
      "loss: 0.144729 [  900/ 1260]\n",
      "loss: 0.169519 [  990/ 1260]\n",
      "loss: 0.090546 [ 1080/ 1260]\n",
      "loss: 0.407103 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 42\n",
      "---------------------------------\n",
      "loss: 0.141131 [    0/ 1260]\n",
      "loss: 0.050704 [   90/ 1260]\n",
      "loss: 0.195695 [  180/ 1260]\n",
      "loss: 0.132523 [  270/ 1260]\n",
      "loss: 0.123862 [  360/ 1260]\n",
      "loss: 0.180577 [  450/ 1260]\n",
      "loss: 0.070413 [  540/ 1260]\n",
      "loss: 0.092445 [  630/ 1260]\n",
      "loss: 0.057582 [  720/ 1260]\n",
      "loss: 0.132525 [  810/ 1260]\n",
      "loss: 0.154436 [  900/ 1260]\n",
      "loss: 0.221074 [  990/ 1260]\n",
      "loss: 0.096772 [ 1080/ 1260]\n",
      "loss: 0.190777 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 43\n",
      "---------------------------------\n",
      "loss: 0.204662 [    0/ 1260]\n",
      "loss: 0.180888 [   90/ 1260]\n",
      "loss: 0.227275 [  180/ 1260]\n",
      "loss: 0.111578 [  270/ 1260]\n",
      "loss: 0.068711 [  360/ 1260]\n",
      "loss: 0.173235 [  450/ 1260]\n",
      "loss: 0.091289 [  540/ 1260]\n",
      "loss: 0.180006 [  630/ 1260]\n",
      "loss: 0.175000 [  720/ 1260]\n",
      "loss: 0.159264 [  810/ 1260]\n",
      "loss: 0.056121 [  900/ 1260]\n",
      "loss: 0.456176 [  990/ 1260]\n",
      "loss: 0.139530 [ 1080/ 1260]\n",
      "loss: 0.058874 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 44\n",
      "---------------------------------\n",
      "loss: 0.037920 [    0/ 1260]\n",
      "loss: 0.147227 [   90/ 1260]\n",
      "loss: 0.102517 [  180/ 1260]\n",
      "loss: 0.367746 [  270/ 1260]\n",
      "loss: 0.145993 [  360/ 1260]\n",
      "loss: 0.260268 [  450/ 1260]\n",
      "loss: 0.110625 [  540/ 1260]\n",
      "loss: 0.173109 [  630/ 1260]\n",
      "loss: 0.165872 [  720/ 1260]\n",
      "loss: 0.130634 [  810/ 1260]\n",
      "loss: 0.175085 [  900/ 1260]\n",
      "loss: 0.105523 [  990/ 1260]\n",
      "loss: 0.073766 [ 1080/ 1260]\n",
      "loss: 0.082622 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 45\n",
      "---------------------------------\n",
      "loss: 0.024411 [    0/ 1260]\n",
      "loss: 0.105289 [   90/ 1260]\n",
      "loss: 0.463739 [  180/ 1260]\n",
      "loss: 0.105627 [  270/ 1260]\n",
      "loss: 0.137623 [  360/ 1260]\n",
      "loss: 0.111400 [  450/ 1260]\n",
      "loss: 0.109666 [  540/ 1260]\n",
      "loss: 0.052675 [  630/ 1260]\n",
      "loss: 0.184955 [  720/ 1260]\n",
      "loss: 0.220409 [  810/ 1260]\n",
      "loss: 0.031293 [  900/ 1260]\n",
      "loss: 0.111436 [  990/ 1260]\n",
      "loss: 0.119106 [ 1080/ 1260]\n",
      "loss: 0.430274 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 46\n",
      "---------------------------------\n",
      "loss: 0.073748 [    0/ 1260]\n",
      "loss: 0.044856 [   90/ 1260]\n",
      "loss: 0.190257 [  180/ 1260]\n",
      "loss: 0.203309 [  270/ 1260]\n",
      "loss: 0.305188 [  360/ 1260]\n",
      "loss: 0.099355 [  450/ 1260]\n",
      "loss: 0.141972 [  540/ 1260]\n",
      "loss: 0.135605 [  630/ 1260]\n",
      "loss: 0.234563 [  720/ 1260]\n",
      "loss: 0.113207 [  810/ 1260]\n",
      "loss: 0.087272 [  900/ 1260]\n",
      "loss: 0.189647 [  990/ 1260]\n",
      "loss: 0.222611 [ 1080/ 1260]\n",
      "loss: 0.214274 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 47\n",
      "---------------------------------\n",
      "loss: 0.195186 [    0/ 1260]\n",
      "loss: 0.116778 [   90/ 1260]\n",
      "loss: 0.142857 [  180/ 1260]\n",
      "loss: 0.110257 [  270/ 1260]\n",
      "loss: 0.199089 [  360/ 1260]\n",
      "loss: 0.194455 [  450/ 1260]\n",
      "loss: 0.028622 [  540/ 1260]\n",
      "loss: 0.267410 [  630/ 1260]\n",
      "loss: 0.135841 [  720/ 1260]\n",
      "loss: 0.329510 [  810/ 1260]\n",
      "loss: 0.045855 [  900/ 1260]\n",
      "loss: 0.112523 [  990/ 1260]\n",
      "loss: 0.294453 [ 1080/ 1260]\n",
      "loss: 0.049110 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 48\n",
      "---------------------------------\n",
      "loss: 0.050850 [    0/ 1260]\n",
      "loss: 0.090328 [   90/ 1260]\n",
      "loss: 0.246448 [  180/ 1260]\n",
      "loss: 0.189264 [  270/ 1260]\n",
      "loss: 0.129472 [  360/ 1260]\n",
      "loss: 0.171617 [  450/ 1260]\n",
      "loss: 0.160267 [  540/ 1260]\n",
      "loss: 0.234553 [  630/ 1260]\n",
      "loss: 0.094178 [  720/ 1260]\n",
      "loss: 0.074559 [  810/ 1260]\n",
      "loss: 0.113360 [  900/ 1260]\n",
      "loss: 0.032699 [  990/ 1260]\n",
      "loss: 0.156860 [ 1080/ 1260]\n",
      "loss: 0.131253 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 49\n",
      "---------------------------------\n",
      "loss: 0.194281 [    0/ 1260]\n",
      "loss: 0.110096 [   90/ 1260]\n",
      "loss: 0.183561 [  180/ 1260]\n",
      "loss: 0.092109 [  270/ 1260]\n",
      "loss: 0.072025 [  360/ 1260]\n",
      "loss: 0.266499 [  450/ 1260]\n",
      "loss: 0.110616 [  540/ 1260]\n",
      "loss: 0.076421 [  630/ 1260]\n",
      "loss: 0.130502 [  720/ 1260]\n",
      "loss: 0.133926 [  810/ 1260]\n",
      "loss: 0.067676 [  900/ 1260]\n",
      "loss: 0.084904 [  990/ 1260]\n",
      "loss: 0.256162 [ 1080/ 1260]\n",
      "loss: 0.172173 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Epoch 50\n",
      "---------------------------------\n",
      "loss: 0.024996 [    0/ 1260]\n",
      "loss: 0.252795 [   90/ 1260]\n",
      "loss: 0.059968 [  180/ 1260]\n",
      "loss: 0.226529 [  270/ 1260]\n",
      "loss: 0.161821 [  360/ 1260]\n",
      "loss: 0.139567 [  450/ 1260]\n",
      "loss: 0.241706 [  540/ 1260]\n",
      "loss: 0.118788 [  630/ 1260]\n",
      "loss: 0.208265 [  720/ 1260]\n",
      "loss: 0.055552 [  810/ 1260]\n",
      "loss: 0.238302 [  900/ 1260]\n",
      "loss: 0.187495 [  990/ 1260]\n",
      "loss: 0.482329 [ 1080/ 1260]\n",
      "loss: 0.142397 [ 1170/ 1260]\n",
      "---------------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = TrainSkeletonData(dir, True)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size = batch_size, shuffle= True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    print(\"---------------------------------\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "92.0\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.892017 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\Pose_test\"\n",
    "test_dir = SkeletonDataset(test_dir)\n",
    "test_loader = DataLoader(test_dir)\n",
    "print(len(test_loader.dataset))\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "     \n",
    "    print(correct)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "test_loop(test_loader, model, loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deep_GCN\"\n",
    "model_save_dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\GCN\\\\model_save\"\n",
    "torch.save(model.state_dict(), os.path.join(model_save_dir, f'{model_name}_weight.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from PIL import Image\n",
    "\n",
    "class RawData(Dataset):\n",
    "    def __init__(self, dir):\n",
    "        self.dir = dir\n",
    "        self.images = os.listdir(dir)\n",
    "        self.predictor = openpifpaf.Predictor(checkpoint='shufflenetv2k16')\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_dir = os.path.join(self.dir, self.images[index])\n",
    "        \n",
    "        image = Image.open(image_dir)\n",
    "        \n",
    "        predictions, _, _ = self.predictor.pil_image(image)\n",
    "\n",
    "        if not predictions:\n",
    "            return -1\n",
    "\n",
    "        skt = np.append(predictions[0].data, (predictions[0].data[5] + predictions[0].data[6])/2).reshape(-1, 3)\n",
    "\n",
    "        return torch.tensor(skt), image_dir\n",
    "\n",
    "dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\RE-ID\\\\datasets\\\\DukeMTMC-reID\\\\bounding_box_train\"\n",
    "\n",
    "datasets = RawData(dir = dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "target = np.array(['front', 'back', 'left', 'right'])\n",
    "save_dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\temp\"\n",
    "\n",
    "\n",
    "for t in target:\n",
    "    if not os.path.exists(os.path.join(save_dir, t)):\n",
    "        os.mkdir(os.path.join(save_dir, t))\n",
    "\n",
    "for data in datasets:\n",
    "    if data == -1:\n",
    "        continue\n",
    "\n",
    "    pred = model1(data[0].unsqueeze(0))\n",
    "    image_path = data[1]\n",
    "    image_name = image_path.split('\\\\')[-1]\n",
    "    \n",
    "    \n",
    "    shutil.copyfile(image_path, os.path.join(save_dir, target[pred.argmax()], image_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 19.7854,  14.7728,   0.7215],\n",
       "         [ 21.7721,  12.4053,   0.6782],\n",
       "         [ 17.4371,  12.5962,   0.7018],\n",
       "         [ 25.6621,  12.8388,   0.7861],\n",
       "         [ 14.2054,  13.2253,   0.7428],\n",
       "         [ 32.1866,  24.7745,   0.9703],\n",
       "         [  9.9523,  25.3505,   0.9770],\n",
       "         [ 38.7743,  38.3007,   0.8980],\n",
       "         [  2.5161,  38.0710,   0.7616],\n",
       "         [ 32.0798,  47.6180,   0.7788],\n",
       "         [ 11.1834,  45.8651,   0.6192],\n",
       "         [ 29.2627,  55.1673,   0.9779],\n",
       "         [ 15.1179,  55.5573,   0.9982],\n",
       "         [ 28.0386,  78.5428,   0.9248],\n",
       "         [ 17.2342,  78.3669,   0.9438],\n",
       "         [ 27.8211, 102.7684,   0.8719],\n",
       "         [ 17.7365, 101.9234,   0.9054],\n",
       "         [ 21.0694,  25.0625,   0.9736]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 19.7854,  14.7728,   0.7215],\n",
      "        [ 21.7721,  12.4053,   0.6782],\n",
      "        [ 17.4371,  12.5962,   0.7018],\n",
      "        [ 25.6621,  12.8388,   0.7861],\n",
      "        [ 14.2054,  13.2253,   0.7428],\n",
      "        [ 32.1866,  24.7745,   0.9703],\n",
      "        [  9.9523,  25.3505,   0.9770],\n",
      "        [ 38.7743,  38.3007,   0.8980],\n",
      "        [  2.5161,  38.0710,   0.7616],\n",
      "        [ 32.0798,  47.6180,   0.7788],\n",
      "        [ 11.1834,  45.8651,   0.6192],\n",
      "        [ 29.2627,  55.1673,   0.9779],\n",
      "        [ 15.1179,  55.5573,   0.9982],\n",
      "        [ 28.0386,  78.5428,   0.9248],\n",
      "        [ 17.2342,  78.3669,   0.9438],\n",
      "        [ 27.8211, 102.7684,   0.8719],\n",
      "        [ 17.7365, 101.9234,   0.9054],\n",
      "        [ 21.0694,  25.0625,   0.9736]])\n"
     ]
    }
   ],
   "source": [
    "X_t = train_loader.dataset.__getitem__(0)[0]\n",
    "X_t = torch.tensor(X_t)\n",
    "print(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 19.7854,  14.7728,   0.7215],\n",
       "         [ 21.7721,  12.4053,   0.6782],\n",
       "         [ 17.4371,  12.5962,   0.7018],\n",
       "         [ 25.6621,  12.8388,   0.7861],\n",
       "         [ 14.2054,  13.2253,   0.7428],\n",
       "         [ 32.1866,  24.7745,   0.9703],\n",
       "         [  9.9523,  25.3505,   0.9770],\n",
       "         [ 38.7743,  38.3007,   0.8980],\n",
       "         [  2.5161,  38.0710,   0.7616],\n",
       "         [ 32.0798,  47.6180,   0.7788],\n",
       "         [ 11.1834,  45.8651,   0.6192],\n",
       "         [ 29.2627,  55.1673,   0.9779],\n",
       "         [ 15.1179,  55.5573,   0.9982],\n",
       "         [ 28.0386,  78.5428,   0.9248],\n",
       "         [ 17.2342,  78.3669,   0.9438],\n",
       "         [ 27.8211, 102.7684,   0.8719],\n",
       "         [ 17.7365, 101.9234,   0.9054],\n",
       "         [ 21.0694,  25.0625,   0.9736]],\n",
       "\n",
       "        [[ 19.7854,  14.7728,   0.7215],\n",
       "         [ 21.7721,  12.4053,   0.6782],\n",
       "         [ 17.4371,  12.5962,   0.7018],\n",
       "         [ 25.6621,  12.8388,   0.7861],\n",
       "         [ 14.2054,  13.2253,   0.7428],\n",
       "         [ 32.1866,  24.7745,   0.9703],\n",
       "         [  9.9523,  25.3505,   0.9770],\n",
       "         [ 38.7743,  38.3007,   0.8980],\n",
       "         [  2.5161,  38.0710,   0.7616],\n",
       "         [ 32.0798,  47.6180,   0.7788],\n",
       "         [ 11.1834,  45.8651,   0.6192],\n",
       "         [ 29.2627,  55.1673,   0.9779],\n",
       "         [ 15.1179,  55.5573,   0.9982],\n",
       "         [ 28.0386,  78.5428,   0.9248],\n",
       "         [ 17.2342,  78.3669,   0.9438],\n",
       "         [ 27.8211, 102.7684,   0.8719],\n",
       "         [ 17.7365, 101.9234,   0.9054],\n",
       "         [ 21.0694,  25.0625,   0.9736]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat((X_t.reshape(1, X_t.shape[0], X_t.shape[1]), X_t.reshape(1, X_t.shape[0], X_t.shape[1])), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[166.5835, 225.6863,   6.9985],\n",
       "         [ 67.2195,  40.0170,   2.1859],\n",
       "         [ 51.4279,  40.5943,   2.1661],\n",
       "         [ 47.4341,  25.2441,   1.4643],\n",
       "         [ 31.6425,  25.8214,   1.4446],\n",
       "         [ 92.0303,  88.1377,   2.8419],\n",
       "         [ 33.5378,  88.4840,   2.7122],\n",
       "         [103.0406, 110.6932,   2.6471],\n",
       "         [ 23.6518, 109.2865,   2.3578],\n",
       "         [ 70.8540,  85.9187,   1.6768],\n",
       "         [ 13.6995,  83.9361,   1.3808],\n",
       "         [ 78.3707, 158.7726,   2.8763],\n",
       "         [ 53.4215, 158.9867,   2.9156],\n",
       "         [ 85.1223, 236.4785,   2.7746],\n",
       "         [ 50.0886, 235.8476,   2.8474],\n",
       "         [ 55.8596, 181.3112,   1.7967],\n",
       "         [ 34.9706, 180.2903,   1.8492],\n",
       "         [127.3744, 200.6849,   5.6184]],\n",
       "\n",
       "        [[166.5835, 225.6863,   6.9985],\n",
       "         [ 67.2195,  40.0170,   2.1859],\n",
       "         [ 51.4279,  40.5943,   2.1661],\n",
       "         [ 47.4341,  25.2441,   1.4643],\n",
       "         [ 31.6425,  25.8214,   1.4446],\n",
       "         [ 92.0303,  88.1377,   2.8419],\n",
       "         [ 33.5378,  88.4840,   2.7122],\n",
       "         [103.0406, 110.6932,   2.6471],\n",
       "         [ 23.6518, 109.2865,   2.3578],\n",
       "         [ 70.8540,  85.9187,   1.6768],\n",
       "         [ 13.6995,  83.9361,   1.3808],\n",
       "         [ 78.3707, 158.7726,   2.8763],\n",
       "         [ 53.4215, 158.9867,   2.9156],\n",
       "         [ 85.1223, 236.4785,   2.7746],\n",
       "         [ 50.0886, 235.8476,   2.8474],\n",
       "         [ 55.8596, 181.3112,   1.7967],\n",
       "         [ 34.9706, 180.2903,   1.8492],\n",
       "         [127.3744, 200.6849,   5.6184]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(A.float(),torch.concat((X_t.reshape(1, X_t.shape[0], X_t.shape[1]), X_t.reshape(1, X_t.shape[0], X_t.shape[1])), axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1f9c1b220c8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[166.5835, 225.6864,   6.9985],\n",
       "        [ 67.2195,  40.0170,   2.1859],\n",
       "        [ 51.4279,  40.5943,   2.1661],\n",
       "        [ 47.4341,  25.2441,   1.4643],\n",
       "        [ 31.6425,  25.8214,   1.4446],\n",
       "        [ 92.0303,  88.1377,   2.8419],\n",
       "        [ 33.5378,  88.4840,   2.7122],\n",
       "        [103.0406, 110.6932,   2.6471],\n",
       "        [ 23.6518, 109.2865,   2.3578],\n",
       "        [ 70.8540,  85.9187,   1.6768],\n",
       "        [ 13.6995,  83.9361,   1.3808],\n",
       "        [ 78.3707, 158.7726,   2.8763],\n",
       "        [ 53.4215, 158.9867,   2.9156],\n",
       "        [ 85.1223, 236.4785,   2.7746],\n",
       "        [ 50.0886, 235.8476,   2.8474],\n",
       "        [ 55.8596, 181.3112,   1.7967],\n",
       "        [ 34.9706, 180.2903,   1.8492],\n",
       "        [127.3744, 200.6849,   5.6184]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(A.float(), X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type_as(): argument 'other' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6804\\904716944.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: type_as(): argument 'other' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "print(A.type_as('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = train_loader.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 19.7854,  14.7728,   0.7215],\n",
       "        [ 21.7721,  12.4053,   0.6782],\n",
       "        [ 17.4371,  12.5962,   0.7018],\n",
       "        [ 25.6621,  12.8388,   0.7861],\n",
       "        [ 14.2054,  13.2253,   0.7428],\n",
       "        [ 32.1866,  24.7745,   0.9703],\n",
       "        [  9.9523,  25.3505,   0.9770],\n",
       "        [ 38.7743,  38.3007,   0.8980],\n",
       "        [  2.5161,  38.0710,   0.7616],\n",
       "        [ 32.0798,  47.6180,   0.7788],\n",
       "        [ 11.1834,  45.8651,   0.6192],\n",
       "        [ 29.2627,  55.1673,   0.9779],\n",
       "        [ 15.1179,  55.5573,   0.9982],\n",
       "        [ 28.0386,  78.5428,   0.9248],\n",
       "        [ 17.2342,  78.3669,   0.9438],\n",
       "        [ 27.8211, 102.7684,   0.8719],\n",
       "        [ 17.7365, 101.9234,   0.9054],\n",
       "        [ 21.0694,  25.0625,   0.9736]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(X_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 19.785437  ,  14.772844  ,   0.7215067 ],\n",
       "        [ 21.772055  ,  12.4052925 ,   0.67824465],\n",
       "        [ 17.437067  ,  12.596153  ,   0.70178014],\n",
       "        [ 25.66205   ,  12.838848  ,   0.7861026 ],\n",
       "        [ 14.205389  ,  13.225267  ,   0.74279624],\n",
       "        [ 32.186584  ,  24.774513  ,   0.97027415],\n",
       "        [  9.952294  ,  25.350466  ,   0.97698075],\n",
       "        [ 38.774258  ,  38.300705  ,   0.8980057 ],\n",
       "        [  2.5161135 ,  38.071003  ,   0.7615855 ],\n",
       "        [ 32.079758  ,  47.617958  ,   0.7787828 ],\n",
       "        [ 11.183401  ,  45.865063  ,   0.6192074 ],\n",
       "        [ 29.262691  ,  55.167286  ,   0.97787946],\n",
       "        [ 15.1179085 ,  55.557304  ,   0.9981807 ],\n",
       "        [ 28.03856   ,  78.54279   ,   0.9248307 ],\n",
       "        [ 17.234167  ,  78.36692   ,   0.94381   ],\n",
       "        [ 27.821056  , 102.76838   ,   0.87185735],\n",
       "        [ 17.736483  , 101.92338   ,   0.9053639 ],\n",
       "        [ 21.069439  ,  25.062489  ,   0.97362745]], dtype=float32),\n",
       " tensor([1., 0., 0., 0.]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.SkeletonDataset at 0x2c8c8be9e88>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) tensor(0) tensor(False)\n",
      "tensor(0) tensor(0) tensor(True)\n",
      "tensor(0) tensor(0) tensor(True)\n",
      "tensor(0) tensor(0) tensor(True)\n",
      "tensor(0) tensor(0) tensor(True)\n",
      "tensor(0) tensor(0) tensor(True)\n",
      "tensor(3) tensor(1) tensor(False)\n",
      "tensor(0) tensor(1) tensor(False)\n",
      "tensor(3) tensor(1) tensor(False)\n",
      "tensor(3) tensor(1) tensor(False)\n",
      "tensor(0) tensor(1) tensor(False)\n",
      "tensor(2) tensor(1) tensor(False)\n",
      "tensor(1) tensor(2) tensor(False)\n",
      "tensor(1) tensor(2) tensor(False)\n",
      "tensor(1) tensor(2) tensor(False)\n",
      "tensor(2) tensor(2) tensor(True)\n",
      "tensor(2) tensor(2) tensor(True)\n",
      "tensor(2) tensor(2) tensor(True)\n",
      "tensor(1) tensor(3) tensor(False)\n",
      "tensor(3) tensor(3) tensor(True)\n",
      "tensor(3) tensor(3) tensor(True)\n",
      "tensor(3) tensor(3) tensor(True)\n",
      "tensor(3) tensor(3) tensor(True)\n",
      "tensor(3) tensor(3) tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(24):\n",
    "    X_t = test_loader.dataset.__getitem__(i)\n",
    "    print(model(torch.tensor(X_t[0]).reshape(-1, X_t[0].shape[0], X_t[0].shape[1])).argmax(), X_t[1].argmax(), model(torch.tensor(X_t[0]).reshape(-1, X_t[0].shape[0], X_t[0].shape[1])).argmax()== X_t[1].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.7854, 14.7728,  0.7215])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\RE-ID_3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got 18, 18x18,3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14028\\89280516.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\RE-ID_3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\GCN\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\RE-ID_3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RE-ID_3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RE-ID_3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\GCN\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, got 18, 18x18,3"
     ]
    }
   ],
   "source": [
    "model(torch.tensor(X_t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(['front', 'back', 'left', 'right']) == X_t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"./model_save\"\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(model_save_dir, 'model_weights.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): GCN_layer(\n",
      "      (fc): Linear(in_features=3, out_features=4, bias=True)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): GCN_layer(\n",
      "      (fc): Linear(in_features=4, out_features=8, bias=True)\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): GCN_layer(\n",
      "      (fc): Linear(in_features=8, out_features=16, bias=True)\n",
      "    )\n",
      "    (5): ReLU()\n",
      "    (6): GCN_layer(\n",
      "      (fc): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (7): ReLU()\n",
      "    (8): GCN_layer(\n",
      "      (fc): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (9): ReLU()\n",
      "    (10): GCN_layer(\n",
      "      (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "    )\n",
      "    (11): ReLU()\n",
      "    (12): GCN_layer(\n",
      "      (fc): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (13): ReLU()\n",
      "    (14): Flatten(start_dim=1, end_dim=-1)\n",
      "    (15): Linear(in_features=576, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "learning_rate : 0.001\n",
      "batch_size : 32\n",
      "epochs : 100\n",
      "Epoch 1\n",
      "---------------------------------\n",
      "loss: 1095.698360 [    0/ 1260]\n",
      "loss: 17.066849 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "---------------------------------\n",
      "loss: 16.920882 [    0/ 1260]\n",
      "loss: 8.458279 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "---------------------------------\n",
      "loss: 6.010592 [    0/ 1260]\n",
      "loss: 4.148694 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "---------------------------------\n",
      "loss: 3.079271 [    0/ 1260]\n",
      "loss: 1.568644 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "---------------------------------\n",
      "loss: 2.245056 [    0/ 1260]\n",
      "loss: 2.602532 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "---------------------------------\n",
      "loss: 2.270326 [    0/ 1260]\n",
      "loss: 3.328323 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "---------------------------------\n",
      "loss: 1.624882 [    0/ 1260]\n",
      "loss: 1.874313 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "---------------------------------\n",
      "loss: 1.910080 [    0/ 1260]\n",
      "loss: 0.950554 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "---------------------------------\n",
      "loss: 1.886442 [    0/ 1260]\n",
      "loss: 1.459791 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "---------------------------------\n",
      "loss: 2.319422 [    0/ 1260]\n",
      "loss: 1.323411 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 11\n",
      "---------------------------------\n",
      "loss: 3.075112 [    0/ 1260]\n",
      "loss: 1.507870 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 12\n",
      "---------------------------------\n",
      "loss: 0.891668 [    0/ 1260]\n",
      "loss: 0.840848 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 13\n",
      "---------------------------------\n",
      "loss: 0.730594 [    0/ 1260]\n",
      "loss: 0.904937 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 14\n",
      "---------------------------------\n",
      "loss: 2.157356 [    0/ 1260]\n",
      "loss: 0.973144 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 15\n",
      "---------------------------------\n",
      "loss: 1.504540 [    0/ 1260]\n",
      "loss: 0.607082 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 16\n",
      "---------------------------------\n",
      "loss: 1.109724 [    0/ 1260]\n",
      "loss: 0.636942 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 17\n",
      "---------------------------------\n",
      "loss: 0.819086 [    0/ 1260]\n",
      "loss: 0.611858 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 18\n",
      "---------------------------------\n",
      "loss: 0.540341 [    0/ 1260]\n",
      "loss: 0.452332 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 19\n",
      "---------------------------------\n",
      "loss: 0.780460 [    0/ 1260]\n",
      "loss: 0.471544 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 20\n",
      "---------------------------------\n",
      "loss: 0.556835 [    0/ 1260]\n",
      "loss: 1.623182 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 21\n",
      "---------------------------------\n",
      "loss: 0.649639 [    0/ 1260]\n",
      "loss: 1.167115 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 22\n",
      "---------------------------------\n",
      "loss: 1.627659 [    0/ 1260]\n",
      "loss: 0.554338 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 23\n",
      "---------------------------------\n",
      "loss: 1.259798 [    0/ 1260]\n",
      "loss: 0.482816 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 24\n",
      "---------------------------------\n",
      "loss: 1.805126 [    0/ 1260]\n",
      "loss: 0.478299 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 25\n",
      "---------------------------------\n",
      "loss: 0.554217 [    0/ 1260]\n",
      "loss: 0.873691 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 26\n",
      "---------------------------------\n",
      "loss: 0.644239 [    0/ 1260]\n",
      "loss: 0.964755 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 27\n",
      "---------------------------------\n",
      "loss: 0.761961 [    0/ 1260]\n",
      "loss: 0.529102 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 28\n",
      "---------------------------------\n",
      "loss: 0.898645 [    0/ 1260]\n",
      "loss: 0.821342 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 29\n",
      "---------------------------------\n",
      "loss: 1.225757 [    0/ 1260]\n",
      "loss: 0.687395 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 30\n",
      "---------------------------------\n",
      "loss: 0.627038 [    0/ 1260]\n",
      "loss: 0.609421 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 31\n",
      "---------------------------------\n",
      "loss: 0.960422 [    0/ 1260]\n",
      "loss: 0.562845 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 32\n",
      "---------------------------------\n",
      "loss: 0.535445 [    0/ 1260]\n",
      "loss: 0.725445 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 33\n",
      "---------------------------------\n",
      "loss: 0.422365 [    0/ 1260]\n",
      "loss: 0.726340 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 34\n",
      "---------------------------------\n",
      "loss: 1.156320 [    0/ 1260]\n",
      "loss: 0.512821 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 35\n",
      "---------------------------------\n",
      "loss: 1.061944 [    0/ 1260]\n",
      "loss: 1.448629 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 36\n",
      "---------------------------------\n",
      "loss: 1.041973 [    0/ 1260]\n",
      "loss: 0.429909 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 37\n",
      "---------------------------------\n",
      "loss: 0.408348 [    0/ 1260]\n",
      "loss: 0.379370 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 38\n",
      "---------------------------------\n",
      "loss: 0.668692 [    0/ 1260]\n",
      "loss: 1.065567 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 39\n",
      "---------------------------------\n",
      "loss: 0.475041 [    0/ 1260]\n",
      "loss: 1.379334 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 40\n",
      "---------------------------------\n",
      "loss: 0.495730 [    0/ 1260]\n",
      "loss: 0.584399 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 41\n",
      "---------------------------------\n",
      "loss: 0.973118 [    0/ 1260]\n",
      "loss: 0.541847 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 42\n",
      "---------------------------------\n",
      "loss: 0.609470 [    0/ 1260]\n",
      "loss: 0.522913 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 43\n",
      "---------------------------------\n",
      "loss: 0.691720 [    0/ 1260]\n",
      "loss: 0.530060 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 44\n",
      "---------------------------------\n",
      "loss: 0.572319 [    0/ 1260]\n",
      "loss: 0.919309 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 45\n",
      "---------------------------------\n",
      "loss: 0.344138 [    0/ 1260]\n",
      "loss: 0.482273 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 46\n",
      "---------------------------------\n",
      "loss: 0.765007 [    0/ 1260]\n",
      "loss: 0.312151 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 47\n",
      "---------------------------------\n",
      "loss: 0.653535 [    0/ 1260]\n",
      "loss: 0.449628 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 48\n",
      "---------------------------------\n",
      "loss: 0.784378 [    0/ 1260]\n",
      "loss: 0.447181 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 49\n",
      "---------------------------------\n",
      "loss: 0.518247 [    0/ 1260]\n",
      "loss: 0.402917 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 50\n",
      "---------------------------------\n",
      "loss: 0.665719 [    0/ 1260]\n",
      "loss: 0.529993 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 51\n",
      "---------------------------------\n",
      "loss: 0.750512 [    0/ 1260]\n",
      "loss: 0.608986 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 52\n",
      "---------------------------------\n",
      "loss: 0.558944 [    0/ 1260]\n",
      "loss: 0.535367 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 53\n",
      "---------------------------------\n",
      "loss: 0.830288 [    0/ 1260]\n",
      "loss: 0.559425 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 54\n",
      "---------------------------------\n",
      "loss: 0.480816 [    0/ 1260]\n",
      "loss: 0.503823 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 55\n",
      "---------------------------------\n",
      "loss: 0.495661 [    0/ 1260]\n",
      "loss: 0.463155 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 56\n",
      "---------------------------------\n",
      "loss: 0.689231 [    0/ 1260]\n",
      "loss: 0.547442 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 57\n",
      "---------------------------------\n",
      "loss: 0.487777 [    0/ 1260]\n",
      "loss: 0.825350 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 58\n",
      "---------------------------------\n",
      "loss: 0.852490 [    0/ 1260]\n",
      "loss: 0.434871 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 59\n",
      "---------------------------------\n",
      "loss: 0.383904 [    0/ 1260]\n",
      "loss: 0.500618 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 60\n",
      "---------------------------------\n",
      "loss: 0.585548 [    0/ 1260]\n",
      "loss: 0.545519 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 61\n",
      "---------------------------------\n",
      "loss: 0.509936 [    0/ 1260]\n",
      "loss: 0.567311 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 62\n",
      "---------------------------------\n",
      "loss: 0.573079 [    0/ 1260]\n",
      "loss: 0.574365 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 63\n",
      "---------------------------------\n",
      "loss: 0.227436 [    0/ 1260]\n",
      "loss: 0.521054 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 64\n",
      "---------------------------------\n",
      "loss: 0.604699 [    0/ 1260]\n",
      "loss: 0.684270 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 65\n",
      "---------------------------------\n",
      "loss: 0.684095 [    0/ 1260]\n",
      "loss: 0.542668 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 66\n",
      "---------------------------------\n",
      "loss: 0.595942 [    0/ 1260]\n",
      "loss: 0.524234 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 67\n",
      "---------------------------------\n",
      "loss: 0.482025 [    0/ 1260]\n",
      "loss: 0.455526 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 68\n",
      "---------------------------------\n",
      "loss: 0.424044 [    0/ 1260]\n",
      "loss: 0.680628 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 69\n",
      "---------------------------------\n",
      "loss: 0.498607 [    0/ 1260]\n",
      "loss: 0.354689 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 70\n",
      "---------------------------------\n",
      "loss: 0.465523 [    0/ 1260]\n",
      "loss: 0.630875 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 71\n",
      "---------------------------------\n",
      "loss: 0.549022 [    0/ 1260]\n",
      "loss: 0.495717 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 72\n",
      "---------------------------------\n",
      "loss: 0.870915 [    0/ 1260]\n",
      "loss: 0.464879 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 73\n",
      "---------------------------------\n",
      "loss: 0.673099 [    0/ 1260]\n",
      "loss: 0.482668 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 74\n",
      "---------------------------------\n",
      "loss: 0.794437 [    0/ 1260]\n",
      "loss: 0.614851 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 75\n",
      "---------------------------------\n",
      "loss: 0.404643 [    0/ 1260]\n",
      "loss: 0.424851 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 76\n",
      "---------------------------------\n",
      "loss: 1.146167 [    0/ 1260]\n",
      "loss: 0.331847 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 77\n",
      "---------------------------------\n",
      "loss: 0.742598 [    0/ 1260]\n",
      "loss: 0.340451 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 78\n",
      "---------------------------------\n",
      "loss: 0.419824 [    0/ 1260]\n",
      "loss: 0.380555 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 79\n",
      "---------------------------------\n",
      "loss: 0.351700 [    0/ 1260]\n",
      "loss: 0.575137 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 80\n",
      "---------------------------------\n",
      "loss: 0.288495 [    0/ 1260]\n",
      "loss: 0.532788 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 81\n",
      "---------------------------------\n",
      "loss: 0.536328 [    0/ 1260]\n",
      "loss: 0.954376 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 82\n",
      "---------------------------------\n",
      "loss: 0.768409 [    0/ 1260]\n",
      "loss: 0.498398 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 83\n",
      "---------------------------------\n",
      "loss: 0.550161 [    0/ 1260]\n",
      "loss: 0.287889 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 84\n",
      "---------------------------------\n",
      "loss: 0.349992 [    0/ 1260]\n",
      "loss: 0.932476 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 85\n",
      "---------------------------------\n",
      "loss: 0.966862 [    0/ 1260]\n",
      "loss: 0.741778 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 86\n",
      "---------------------------------\n",
      "loss: 0.465644 [    0/ 1260]\n",
      "loss: 0.358276 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 87\n",
      "---------------------------------\n",
      "loss: 0.424933 [    0/ 1260]\n",
      "loss: 0.454293 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 88\n",
      "---------------------------------\n",
      "loss: 0.580587 [    0/ 1260]\n",
      "loss: 0.482112 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 89\n",
      "---------------------------------\n",
      "loss: 0.682438 [    0/ 1260]\n",
      "loss: 0.388150 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 90\n",
      "---------------------------------\n",
      "loss: 0.594548 [    0/ 1260]\n",
      "loss: 0.721262 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 91\n",
      "---------------------------------\n",
      "loss: 0.403335 [    0/ 1260]\n",
      "loss: 0.490365 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 92\n",
      "---------------------------------\n",
      "loss: 0.715746 [    0/ 1260]\n",
      "loss: 0.407251 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 93\n",
      "---------------------------------\n",
      "loss: 0.234440 [    0/ 1260]\n",
      "loss: 0.560805 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 94\n",
      "---------------------------------\n",
      "loss: 0.454111 [    0/ 1260]\n",
      "loss: 0.284002 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 95\n",
      "---------------------------------\n",
      "loss: 0.441483 [    0/ 1260]\n",
      "loss: 0.580027 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 96\n",
      "---------------------------------\n",
      "loss: 0.744485 [    0/ 1260]\n",
      "loss: 0.411563 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 97\n",
      "---------------------------------\n",
      "loss: 0.570048 [    0/ 1260]\n",
      "loss: 0.763035 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 98\n",
      "---------------------------------\n",
      "loss: 0.610142 [    0/ 1260]\n",
      "loss: 0.393548 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 99\n",
      "---------------------------------\n",
      "loss: 0.386876 [    0/ 1260]\n",
      "loss: 0.208488 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Epoch 100\n",
      "---------------------------------\n",
      "loss: 0.493926 [    0/ 1260]\n",
      "loss: 0.216723 [ 1024/ 1260]\n",
      "---------------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import GCN\n",
    "import torch.nn as  nn\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\pose-angle\"\n",
    "model_save_dir = \"./model_save\"\n",
    "model_name = \"model_05_12_13_43\"\n",
    "\n",
    "dataset = TrainSkeletonData(dir, make_skeleton=False)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size = 32, shuffle= True)\n",
    "\n",
    "\n",
    "A =\\\n",
    "[\n",
    "    [1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], #11\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1], #12\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0], #13\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0], #14\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], #15\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], #16\n",
    "    [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]  #17\n",
    "]\n",
    "\n",
    "A = torch.tensor(A).float()\n",
    "in_feature = 3\n",
    "num_class = 4\n",
    "model1 = GCN(in_feature, num_class, A)\n",
    "print(model1)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size= 32\n",
    "epochs = 100\n",
    "print(f\"learning_rate : {learning_rate}\")\n",
    "print(f\"batch_size : {batch_size}\")\n",
    "print(f\"epochs : {epochs}\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate)    \n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------------------\")\n",
    "    size = len(train_loader.dataset)\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        pred = model1(X)\n",
    "        loss=  loss_fn(pred,y)\n",
    "\n",
    "        #Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 32 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model1.state_dict(), os.path.join(model_save_dir, f'{model_name}_weight.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 Test 데이터 개수 :  99\n",
      "------------------------------\n",
      "log : skeleton_error\n",
      "------------------------------\n",
      "맞은 개수 :  87.0\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.335525 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import GCN\n",
    "import torch.nn as  nn\n",
    "from torchvision import transforms\n",
    "\n",
    "test_dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\Pose_test\"\n",
    "test_datasets = TestDataset(test_dir)\n",
    "\n",
    "print(\"전체 Test 데이터 개수 : \", len(test_datasets))\n",
    "def test_loop(dataset, model, loss_fn):\n",
    "    size = len(dataset)\n",
    "    test_loss, correct, wrong = 0, 0, 0\n",
    "    model.eval()\n",
    "    print(\"------------------------------\")\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataset):\n",
    "            if data == -1:\n",
    "                print('log : skeleton_error')\n",
    "                wrong += 1\n",
    "                continue\n",
    "            X,y = torch.tensor(data[0]).unsqueeze(0), data[1].unsqueeze(0)\n",
    "            \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    print(\"------------------------------\")\n",
    "    print(\"맞은 개수 : \", correct)\n",
    "\n",
    "    test_loss /= (size -wrong)\n",
    "    correct /= (size -wrong)\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "test_loop(test_datasets, model1, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TestDataset' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21024\\1704533085.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_datasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21024\\1181156315.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TestDataset' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "test_loop(test_datasets, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f69f9d2e86e3673dbd4427f21370f94ad4c3b7d880a1660296573ae1db7f643"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('RE-ID_3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
